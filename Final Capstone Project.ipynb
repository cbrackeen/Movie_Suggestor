{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import random\n",
    "import os\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "%matplotlib inline\n",
    "\n",
    "comedy = pd.read_json('CMovies.json', orient='records')\n",
    "horror = pd.read_json('HorMovies.json', orient='records')\n",
    "action = pd.read_json('ActMovies.json', orient='records')\n",
    "crime = pd.read_json('CriMovies.json', orient='records')\n",
    "drama = pd.read_json('DraMovies.json', orient='records')\n",
    "family = pd.read_json('FamMovies.json', orient='records')\n",
    "history = pd.read_json('HisMovies.json', orient='records')\n",
    "romance = pd.read_json('RomMovies.json', orient='records')\n",
    "scifi = pd.read_json('SFMovies.json', orient='records')\n",
    "thriller = pd.read_json('ThrMovies.json', orient='records')\n",
    "western = pd.read_json('WesMovies.json', orient='records')\n",
    "\n",
    "# Importing movie titles\n",
    "comedy_title = pd.read_json('Comedy.json', orient='records')\n",
    "horror_title = pd.read_json('Horror.json', orient='records')\n",
    "action_title = pd.read_json('Action.json', orient='records')\n",
    "crime_title = pd.read_json('Crime.json', orient='records')\n",
    "drama_title = pd.read_json('Drama.json', orient='records')\n",
    "family_title = pd.read_json('Family.json', orient='records')\n",
    "history_title = pd.read_json('History.json', orient='records')\n",
    "romance_title = pd.read_json('Romance.json', orient='records')\n",
    "scifi_title = pd.read_json('SciFi.json', orient='records')\n",
    "thriller_title = pd.read_json('Thriller.json', orient='records')\n",
    "western_title = pd.read_json('Western.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(lst):\n",
    "    for item in lst:\n",
    "        # Clean up year information\n",
    "        item.Year = item.Year.apply(lambda x: str(x).replace('â€“','') )\n",
    "        item.Year = item.Year.apply(lambda x: int(x))\n",
    "        \n",
    "        #Get rid of N/A in Plot and rating\n",
    "        item.Plot = item.Plot.apply(lambda x: str(x).replace('N/A', ''))\n",
    "        item.Rating = item.Rating.apply(lambda x: str(x).replace('N/A', '0'))\n",
    "        item.Rating = item.Rating.apply(lambda x: float(x))\n",
    "        \n",
    "    return lst\n",
    "\n",
    "genre_data = [comedy,horror,action,crime,drama,family,history,romance,scifi,thriller,western]\n",
    "genre_data = clean_data(genre_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use plot, rating, and year for all of our classifiers\n",
    "vectorizer = TfidfVectorizer(lowercase=False, \n",
    "                          stop_words=None,\n",
    "                          ngram_range=(1, 1), \n",
    "                      \n",
    "                          max_df=.5, \n",
    "                          min_df=1,\n",
    "                          max_features=None, \n",
    "                          vocabulary=None, \n",
    "                          binary=False)\n",
    "\n",
    "comPlot = vectorizer.fit_transform(comedy.Plot)\n",
    "com_terms = vectorizer.get_feature_names()\n",
    "horPlot = vectorizer.fit_transform(horror.Plot)\n",
    "hor_terms = vectorizer.get_feature_names()\n",
    "famPlot = vectorizer.fit_transform(family.Plot)\n",
    "fam_terms = vectorizer.get_feature_names()\n",
    "actPlot = vectorizer.fit_transform(action.Plot)\n",
    "act_terms = vectorizer.get_feature_names()\n",
    "criPlot = vectorizer.fit_transform(crime.Plot)\n",
    "cri_terms = vectorizer.get_feature_names()\n",
    "draPlot = vectorizer.fit_transform(drama.Plot)\n",
    "dra_terms = vectorizer.get_feature_names()\n",
    "hisPlot = vectorizer.fit_transform(history.Plot)\n",
    "his_terms = vectorizer.get_feature_names()\n",
    "romPlot = vectorizer.fit_transform(romance.Plot)\n",
    "rom_terms = vectorizer.get_feature_names()\n",
    "sfPlot = vectorizer.fit_transform(scifi.Plot)\n",
    "sf_terms = vectorizer.get_feature_names()\n",
    "thrPlot = vectorizer.fit_transform(thriller.Plot)\n",
    "thr_terms = vectorizer.get_feature_names()\n",
    "wesPlot = vectorizer.fit_transform(western.Plot)\n",
    "wes_terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comedy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for drama is 0.8387096774193549.\n",
      "The score for crime is 0.88.\n"
     ]
    }
   ],
   "source": [
    "# Create data for whether the movie falls under that genre (even as subgenre)\n",
    "def IsGenre(data,genre):\n",
    "    isGenre = []\n",
    "    for item in data.Genre:\n",
    "        item = item.replace(',','')\n",
    "        check = item.split(' ')\n",
    "        if genre in check:\n",
    "            isGenre.append(1)\n",
    "        else:\n",
    "            isGenre.append(0)\n",
    "    return isGenre\n",
    "\n",
    "# Process the plot data and create new dataframe with the processed data\n",
    "def create_data(vect, data, genre):\n",
    "    tfidf = pd.DataFrame(vect.todense()).sum(axis=1)\n",
    "    \n",
    "    isGenre = pd.DataFrame(IsGenre(data, genre))\n",
    "    \n",
    "    # Combine all created data to final dataframe\n",
    "    new = pd.concat([tfidf, isGenre, data.loc[:,['Rating','Year']]], axis=1)\n",
    "    new.columns = ['tfidf','isGenre','Rating','Year']\n",
    "    return new\n",
    "\n",
    "# Creating all of the data sets for comedy training\n",
    "def create_training(gen):\n",
    "    comedy_pro = create_data(comPlot, comedy, gen)\n",
    "    horror_pro = create_data(horPlot, horror, gen)\n",
    "    family_pro = create_data(famPlot, family, gen)\n",
    "    action_pro = create_data(actPlot, action, gen)\n",
    "    crime_pro = create_data(criPlot, crime, gen)\n",
    "    drama_pro = create_data(draPlot, drama, gen)\n",
    "    history_pro = create_data(hisPlot, history, gen)\n",
    "    romance_pro = create_data(romPlot, romance, gen)\n",
    "    scifi_pro = create_data(sfPlot, scifi, gen)\n",
    "    thriller_pro = create_data(thrPlot, thriller, gen)\n",
    "    western_pro = create_data(wesPlot, western, gen)\n",
    "    return [comedy_pro, horror_pro, family_pro, action_pro,crime_pro,drama_pro,history_pro,romance_pro,\\\n",
    "           scifi_pro,thriller_pro,western_pro]\n",
    "\n",
    "#Creating the master dataset to use for training\n",
    "com_data = create_training('Comedy')\n",
    "x_com = pd.concat(com_data[:4])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_com = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "def fit_data(lst, data):\n",
    "    for item in lst:\n",
    "        y_train = data.isGenre\n",
    "        x_train = data.drop(columns=['isGenre'])\n",
    "        item.fit(x_train, y_train)\n",
    "\n",
    "fit_data([rfc_com], x_com)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "\n",
    "def test_data(rfc_10, ind, data):\n",
    "    names = ['comedy','horror','family','action','crime','drama',\\\n",
    "             'history','romance','scifi','thriller','western']\n",
    "    gen = names[ind]\n",
    "    \n",
    "    x_test = data[ind].drop(columns=['isGenre'])\n",
    "    y_test = data[ind].isGenre\n",
    "    print('The score for {} is {}.'.format(gen,rfc_10.score(x_test,y_test)))\n",
    "\n",
    "# Drama\n",
    "test_data(rfc_com, 5, com_data)\n",
    "\n",
    "#Crime\n",
    "test_data(rfc_com, 4, com_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horror Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for crime is 0.94.\n",
      "The score for drama is 0.967741935483871.\n"
     ]
    }
   ],
   "source": [
    "#Creating the master dataset to use for training\n",
    "hor_data = create_training('Horror')\n",
    "x_hor = pd.concat(hor_data[:4])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_hor = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "#Fitting Data\n",
    "fit_data([rfc_hor], x_hor)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "test_data(rfc_hor, 4, hor_data)\n",
    "test_data(rfc_hor, 5, hor_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for drama is 1.0.\n",
      "The score for scifi is 0.94.\n"
     ]
    }
   ],
   "source": [
    "#Creating the master dataset to use for training\n",
    "fam_data = create_training('Family')\n",
    "x_fam = pd.concat(fam_data[0:5])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_fam = RandomForestClassifier(max_depth=10)\n",
    "    \n",
    "fit_data([rfc_fam], x_fam)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "test_data(rfc_fam, 5, fam_data)\n",
    "test_data(rfc_fam, 8, fam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.94.\n"
     ]
    }
   ],
   "source": [
    "#Creating the master dataset to use for training\n",
    "act_data = create_training('Action')\n",
    "x_act = pd.concat(act_data[1:])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_act = RandomForestClassifier(max_depth=10)\n",
    "    \n",
    "fit_data([rfc_act], x_act)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "test_data(rfc_act, 0, act_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.78.\n",
      "The score for scifi is 0.74.\n"
     ]
    }
   ],
   "source": [
    "#Creating the master dataset to use for training\n",
    "cri_data = create_training('Crime')\n",
    "x_cri = pd.concat(cri_data[2:6])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_cri = RandomForestClassifier(max_depth=10)\n",
    "    \n",
    "fit_data([rfc_cri], x_cri)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "test_data(rfc_cri, 0, cri_data)\n",
    "test_data(rfc_cri, 8, cri_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drama Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.76.\n",
      "The score for horror is 0.7254901960784313.\n"
     ]
    }
   ],
   "source": [
    "#Creating the master dataset to use for training\n",
    "dra_data = create_training('Drama')\n",
    "x_dra = pd.concat(dra_data[2:])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_dra = RandomForestClassifier(max_depth=10)\n",
    "    \n",
    "fit_data([rfc_dra], x_dra)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "test_data(rfc_dra, 0, dra_data)\n",
    "test_data(rfc_dra, 1, dra_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for romance is 0.96.\n",
      "The score for scifi is 1.0.\n"
     ]
    }
   ],
   "source": [
    "#Creating the master dataset to use for training\n",
    "his_data = create_training('History')\n",
    "x_his = pd.concat(his_data[0:7])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_his = RandomForestClassifier(max_depth=10)\n",
    "    \n",
    "fit_data([rfc_his], x_his)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "test_data(rfc_his, 7, his_data)\n",
    "test_data(rfc_his, 8, his_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Romance Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.82.\n",
      "The score for scifi is 0.94.\n"
     ]
    }
   ],
   "source": [
    "#Creating the master dataset to use for training\n",
    "rom_data = create_training('Romance')\n",
    "x_rom = pd.concat(cri_data[2:8])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_rom = RandomForestClassifier(max_depth=10)\n",
    "    \n",
    "fit_data([rfc_rom], x_rom)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "test_data(rfc_rom, 0, rom_data)\n",
    "test_data(rfc_rom, 8, rom_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sciene Fiction Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.9.\n",
      "The score for horror is 0.8431372549019608.\n"
     ]
    }
   ],
   "source": [
    "#Creating the master dataset to use for training\n",
    "sf_data = create_training('Sci-Fi')\n",
    "x_sf = pd.concat(sf_data[2:9])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_sf = RandomForestClassifier(max_depth=10)\n",
    "    \n",
    "fit_data([rfc_sf], x_sf)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "test_data(rfc_sf, 0, sf_data)\n",
    "test_data(rfc_sf, 1, sf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thriller Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 1.0.\n",
      "The score for family is 1.0.\n"
     ]
    }
   ],
   "source": [
    "#Creating the master dataset to use for training\n",
    "thr_data = create_training('Thriller')\n",
    "x_thr = pd.concat(thr_data[5:])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_thr = RandomForestClassifier(max_depth=10)\n",
    "    \n",
    "fit_data([rfc_thr], x_thr)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "test_data(rfc_thr, 0, thr_data)\n",
    "test_data(rfc_thr, 2, thr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Western Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.98.\n",
      "The score for horror is 0.9411764705882353.\n"
     ]
    }
   ],
   "source": [
    "#Creating the master dataset to use for training\n",
    "wes_data = create_training('Western')\n",
    "x_wes = pd.concat(wes_data[5:])\n",
    "\n",
    "#Training the comedy random forest classifier\n",
    "rfc_wes = RandomForestClassifier(max_depth=10)\n",
    "    \n",
    "fit_data([rfc_wes], x_wes)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "test_data(rfc_wes, 0, wes_data)\n",
    "test_data(rfc_wes, 1, wes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(data_plot_sent):\n",
    "    parser = spacy.load('en')\n",
    "\n",
    "    # Parsing Gatsby.\n",
    "    plot_doc = parser(data_plot_sent)\n",
    "\n",
    "# Dividing the text into sentences and storing them as a list of strings.\n",
    "    sentences=[]\n",
    "    for span in plot_doc.sents:\n",
    "        # go from the start to the end of each span, returning each token in the sentence\n",
    "        # combine each token using join()\n",
    "        sent = ''.join(plot_doc[i].string for i in range(span.start, span.end)).strip()\n",
    "        sentences.append(sent)\n",
    "    \n",
    "    vect_word = TfidfVectorizer(lowercase=False, \n",
    "                              stop_words='english',\n",
    "                              ngram_range=(1, 1), \n",
    "                              analyzer=u'word', \n",
    "                              min_df=1,\n",
    "                              max_features=None, \n",
    "                              vocabulary=None, \n",
    "                              binary=False)\n",
    "\n",
    "    tfidf = vect_word.fit_transform(sentences)\n",
    "    # Calculating similarity\n",
    "    similarity = tfidf * tfidf.T\n",
    "\n",
    "    # Identifying the sentence with the highest rank.\n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity)\n",
    "    ranks=nx.pagerank(nx_graph, alpha=.85, tol=.00000001)\n",
    "\n",
    "    ranked = sorted(((ranks[i],s) for i,s in enumerate(sentences)),\n",
    "                    reverse=True)\n",
    "\n",
    "\n",
    "    return ranked[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most classifiers are performing very well. We will move on to building the recommender that will classify an input. Likely will include a topic comparison to verify the classifier's accruacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fav_data(fav_info):\n",
    "    vect = TfidfVectorizer(lowercase=False, \n",
    "                          stop_words=None,\n",
    "                          ngram_range=(1, 1), \n",
    "                       \n",
    "                          min_df=1,\n",
    "                          max_features=None, \n",
    "                          vocabulary=None, \n",
    "                          binary=False)\n",
    "    fav_plot = vect.fit_transform(fav_info.Plot)\n",
    "    info = create_data(fav_plot,fav_info, 'Comedy')\n",
    "    return info.drop(columns=['isGenre'])\n",
    "\n",
    "def get_movie_info(titles):\n",
    "    importlist = titles\n",
    "    \n",
    "    #Define crawler and processer\n",
    "    class ImportSpider(scrapy.Spider):\n",
    "        name = \"ImportedMovie\"\n",
    "        start_urls=[]\n",
    "\n",
    "        # Initiating Start URLs\n",
    "        for i in range(len(importlist)):\n",
    "            item = str(importlist[i])\n",
    "            start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "        # Identifying the information we want from the query response and extracting it using xpath.\n",
    "        def parse(self, response):\n",
    "            for item in response.xpath('//movie'):\n",
    "                # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "                yield {\n",
    "                        'Title': item.xpath('@title').extract_first(),\n",
    "                    'Year': item.xpath('@year').extract_first(),\n",
    "                    'Genre': item.xpath('@genre').extract_first(),\n",
    "                    'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "                    'Plot': item.xpath('@plot').extract_first()\n",
    "\n",
    "                }\n",
    "            \n",
    "    process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'ImportedData.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False   \n",
    "    })\n",
    "    \n",
    "    #Delete file if it already exists\n",
    "    if os.path.exists('ImportedData.json'):\n",
    "        os.remove('ImportedData.json')\n",
    "    \n",
    "    \n",
    "    process.crawl(ImportSpider)\n",
    "    process.start()\n",
    "    \n",
    "    info = pd.read_json('ImportedData.json')\n",
    "    data = create_fav_data(info)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the first movie you like?Batman\n",
      "What is the second movie you like?Lego Movie\n",
      "What is the third movie you like?The Avengers\n",
      "\n",
      " You should watch None. \n",
      " Patrick Bateman is handsome, well educated and intelligent.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifiers = [(\"Comedy\", rfc_com), (\"Horror\", rfc_hor),(\"Family\", rfc_fam),(\"Action\",rfc_act),\\\n",
    "              (\"Crime\", rfc_cri),(\"Drama\",rfc_dra),(\"History\",rfc_his),(\"Romance\",rfc_rom),\\\n",
    "              (\"Science Fiction\",rfc_sf),(\"Thriller\",rfc_thr),(\"Western\",rfc_wes)]\n",
    "\n",
    "titles = [(\"Comedy\", comedy_title), (\"Horror\", horror_title),(\"Family\", family_title),(\"Action\",action_title),\\\n",
    "              (\"Crime\", crime_title),(\"Drama\",drama_title),(\"History\",history_title),(\"Romance\",romance_title),\\\n",
    "              (\"Science Fiction\",scifi_title),(\"Thriller\",thriller_title),(\"Western\",western_title)]\n",
    "\n",
    "datas = [(\"Comedy\", comedy), (\"Horror\", horror),(\"Family\", family),(\"Action\",action),\\\n",
    "              (\"Crime\", crime),(\"Drama\",drama),(\"History\",history),(\"Romance\",romance),\\\n",
    "              (\"Science Fiction\",scifi),(\"Thriller\",thriller),(\"Western\",western)]\n",
    "\n",
    "def get_profile(cfrs, test_input):\n",
    "    profile = []\n",
    "    max_input=0\n",
    "    for item in cfrs:\n",
    "        genre = item[0]\n",
    "        cf = item[1]\n",
    "        prediction = cf.predict(test_input)\n",
    "        if prediction.sum() > max_input:\n",
    "            max_input = prediction.sum()\n",
    "    for item in cfrs:\n",
    "        genre = item[0]\n",
    "        cf = item[1]\n",
    "        prediction = cf.predict(test_input)\n",
    "        if prediction.sum() == max_input:\n",
    "            profile.append(genre)\n",
    "\n",
    "    return profile\n",
    "\n",
    "def get_recommendations(cfrs, ttl):\n",
    "    \n",
    "    movie_list = []\n",
    "    \n",
    "    movie_list.append(input(\"What is the first movie you like?\"))\n",
    "    movie_list.append(input(\"What is the second movie you like?\"))\n",
    "    movie_list.append(input(\"What is the third movie you like?\"))\n",
    "    \n",
    "    test_input = x_com.iloc[:4,:].drop(columns=['isGenre'])\n",
    "    actual_input = get_movie_info(movie_list)\n",
    "    \n",
    "    fav_info = actual_input\n",
    "    \n",
    "    profile = get_profile(cfrs, fav_info)\n",
    "    \n",
    "    # Random pick from liked genres\n",
    "    ind = random.randint(0,len(profile)-1)\n",
    "    genre = profile[ind]\n",
    "    \n",
    "    \n",
    "    # Getting title list\n",
    "    for item in ttl:\n",
    "        if item[0] == genre:\n",
    "            ttls = item[1]\n",
    "            break\n",
    "    \n",
    "    # Random pick in appropriate genre\n",
    "    ind2 = random.randint(0,len(ttls)-1)\n",
    "    pick = ttls.iloc[ind2,0]\n",
    "    \n",
    "    for g in datas:\n",
    "        if g[0] == genre:\n",
    "            plot_data = g[1]\n",
    "            \n",
    "    plot_sum = get_summary(plot_data.iloc[ind2,:].Plot)\n",
    "\n",
    "    return pick, plot_sum\n",
    "\n",
    "pick, plot_summ = get_recommendations(classifiers,titles)\n",
    "\n",
    "print(\"\\n You should watch {}. \\n You will like it because: \\n {}\".format(pick, plot_summ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix:\n",
    "### A. Gathering information.\n",
    "Collecting popular titles by genre from Ranker.com via webscraper and collecting the movie data (plot, ranking, and year) from IMDB using their open api OMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class ComedySpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Comedy\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/crowdranked-list/100-all-time-greatest-comedy-films/',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Comedy.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(ComedySpider)\n",
    "# process.start()\n",
    "# print('Comedy Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class HorrorSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Horror\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/crowdranked-list/the-greatest-horror-films-ever-made/',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Horror.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(HorrorSpider)\n",
    "# process.start()\n",
    "# print('Horror Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class ActionSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Action\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/crowdranked-list/best-action-movies',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Action.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(ActionSpider)\n",
    "# process.start()\n",
    "# print('Action Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class CrimeSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Crime\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/all-crime-movies-or-list-of-crime-movies/all-genre-movies-lists',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Crime.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(CrimeSpider)\n",
    "# process.start()\n",
    "# print('Crime Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class DramaSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Drama\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/best-intelligent-dramas/ranker-film?ref=search',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Drama.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(DramaSpider)\n",
    "# process.start()\n",
    "# print('Drama Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class FamilySpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Family\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/best-pg-family-film-movies/reference',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Family.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(FamilySpider)\n",
    "# process.start()\n",
    "# print('Family Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class HistorySpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"History\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/best-pg-13-history-movies/reference',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'History.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(HistorySpider)\n",
    "# process.start()\n",
    "# print('History Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class RomanceSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Romance\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/romance-film-movies-and-films/reference',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Romance.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(RomanceSpider)\n",
    "# process.start()\n",
    "# print('Romance Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class SciFiSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"SciFi\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/classic-science-fiction-movies/ranker-film',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'SciFi.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(SciFiSpider)\n",
    "# process.start()\n",
    "# print('Sci-Fi Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class ThrillerSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Thriller\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/best-pg-thriller-movies/reference',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Thriller.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(ThrillerSpider)\n",
    "# process.start()\n",
    "# print('Thriller Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class WesternSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Western\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/crowdranked-list/the-best-western-movies-ever-made',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Western.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(WesternSpider)\n",
    "# process.start()\n",
    "# print('Western Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class CMSpider(scrapy.Spider):\n",
    "#     name = \"Movie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(comedy_title)):\n",
    "#         item = str(comedy_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'CMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False,\n",
    "#     # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(CMSpider)\n",
    "# process.start()\n",
    "# print('Comedy Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class HorMSpider(scrapy.Spider):\n",
    "#     name = \"HorrorMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(horror_title)):\n",
    "#         item = str(horror_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'HorMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False   \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(HorMSpider)\n",
    "# process.start()\n",
    "# print('Horror Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class ActMSpider(scrapy.Spider):\n",
    "#     name = \"ActionMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(action_title)):\n",
    "#         item = str(action_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'ActMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False  \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(ActMSpider)\n",
    "# process.start()\n",
    "# print('Action Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class CriMSpider(scrapy.Spider):\n",
    "#     name = \"CrimeMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(crime_title)):\n",
    "#         item = str(crime_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'CriMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False   \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(CriMSpider)\n",
    "# process.start()\n",
    "# print('Crime Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class DraMSpider(scrapy.Spider):\n",
    "#     name = \"DramaMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(drama_title)):\n",
    "#         item = str(drama_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'DraMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False    \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(DraMSpider)\n",
    "# process.start()\n",
    "# print('Drama Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class FamMSpider(scrapy.Spider):\n",
    "#     name = \"FamilyMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(family_title)):\n",
    "#         item = str(family_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'FamMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False   \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(FamMSpider)\n",
    "# process.start()\n",
    "# print('Family Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class HisMSpider(scrapy.Spider):\n",
    "#     name = \"HistoryMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(history_title)):\n",
    "#         item = str(history_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'HisMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(HisMSpider)\n",
    "# process.start()\n",
    "# print('History Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class RomMSpider(scrapy.Spider):\n",
    "#     name = \"RomanceMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(romance_title)):\n",
    "#         item = str(romance_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'RomMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False    \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(RomMSpider)\n",
    "# process.start()\n",
    "# print('Romance Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class SFMSpider(scrapy.Spider):\n",
    "#     name = \"SciFiMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(scifi_title)):\n",
    "#         item = str(scifi_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'SFMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False   \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(SFMSpider)\n",
    "# process.start()\n",
    "# print('Sci-Fi Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class ThrMSpider(scrapy.Spider):\n",
    "#     name = \"ThrillerMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(thriller_title)):\n",
    "#         item = str(thriller_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'ThrMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False\n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(ThrMSpider)\n",
    "# process.start()\n",
    "# print('Thriller Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class WesMSpider(scrapy.Spider):\n",
    "#     name = \"WesternMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(western_title)):\n",
    "#         item = str(western_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'WesMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False, \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(WesMSpider)\n",
    "# process.start()\n",
    "# print('Western Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class InputSpider(scrapy.Spider):\n",
    "#     name = \"MovieInput\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     input_titles = ['Batman', 'Lego Movie','Avengers']\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(input_titles)):\n",
    "#         item = str(input_titles[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'MovieInput.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False, \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(InputSpider)\n",
    "# process.start()\n",
    "# print('Movie Input Extracted!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
